{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT8GeBUZ_f9E",
        "outputId": "79bde1f3-8bd1-4dbd-a533-d9d8fdaa9f8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'private'...\n",
            "remote: Enumerating objects: 1521, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 1521 (delta 0), reused 5 (delta 0), pack-reused 1516\u001b[K\n",
            "Receiving objects: 100% (1521/1521), 182.30 MiB | 20.98 MiB/s, done.\n",
            "Resolving deltas: 100% (738/738), done.\n",
            "Updating files: 100% (1655/1655), done.\n",
            "Cloning into 'maril'...\n",
            "remote: Enumerating objects: 1333, done.\u001b[K\n",
            "remote: Counting objects: 100% (1333/1333), done.\u001b[K\n",
            "remote: Compressing objects: 100% (702/702), done.\u001b[K\n",
            "remote: Total 1333 (delta 678), reused 1205 (delta 626), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1333/1333), 948.72 MiB | 20.99 MiB/s, done.\n",
            "Resolving deltas: 100% (678/678), done.\n",
            "Updating files: 100% (1101/1101), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.20.0 typeguard-2.13.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-multi-head\n",
            "  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-self-attention==0.51.0 (from keras-multi-head)\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-self-attention==0.51.0->keras-multi-head) (1.22.4)\n",
            "Building wheels for collected packages: keras-multi-head, keras-self-attention\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14979 sha256=f6ddf03362de84355e4d0e262774719a3b1befb76394f28b2f874d96df781a22\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/23/4b/06d7ae21714f70fcc25b48f972cc8e5e7f4b6b764a038b509d\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18895 sha256=5a39114a2fd9b9fe2a903ad1e630353402d5271c077f3976be1da6b7fc1d17f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/f7/24/607b483144fb9c47b4ba2c5fba6b68e54aeee2d5bf6c05302e\n",
            "Successfully built keras-multi-head keras-self-attention\n",
            "Installing collected packages: keras-self-attention, keras-multi-head\n",
            "Successfully installed keras-multi-head-0.29.0 keras-self-attention-0.51.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2-cp310-cp310-manylinux2014_x86_64.whl (98.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.22.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2022.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.2)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ngboost\n",
            "  Downloading ngboost-0.4.1-py3-none-any.whl (33 kB)\n",
            "Collecting lifelines>=0.25 (from ngboost)\n",
            "  Downloading lifelines-0.27.7-py3-none-any.whl (409 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.4/409.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from ngboost) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.10/dist-packages (from ngboost) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from ngboost) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.7.2 in /usr/local/lib/python3.10/dist-packages (from ngboost) (1.10.1)\n",
            "Requirement already satisfied: tqdm>=4.3 in /usr/local/lib/python3.10/dist-packages (from ngboost) (4.65.0)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from lifelines>=0.25->ngboost) (3.7.1)\n",
            "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.10/dist-packages (from lifelines>=0.25->ngboost) (1.5)\n",
            "Collecting autograd-gamma>=0.3 (from lifelines>=0.25->ngboost)\n",
            "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting formulaic>=0.2.2 (from lifelines>=0.25->ngboost)\n",
            "  Downloading formulaic-0.6.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.4/82.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->ngboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->ngboost) (2022.7.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->ngboost) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->ngboost) (3.1.0)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd>=1.5->lifelines>=0.25->ngboost) (0.18.3)\n",
            "Collecting astor>=0.8 (from formulaic>=0.2.2->lifelines>=0.25->ngboost)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting interface-meta>=1.2.0 (from formulaic>=0.2.2->lifelines>=0.25->ngboost)\n",
            "  Downloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (4.6.3)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (1.14.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.3.5->ngboost) (1.16.0)\n",
            "Building wheels for collected packages: autograd-gamma\n",
            "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4031 sha256=80df2bfcd3e501b7f41dd9ef4d2cfde15d5a8ee2fecb5a02c4948cd232f0ca73\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/cc/e0/ef2969164144c899fedb22b338f6703e2b9cf46eeebf254991\n",
            "Successfully built autograd-gamma\n",
            "Installing collected packages: interface-meta, astor, autograd-gamma, formulaic, lifelines, ngboost\n",
            "Successfully installed astor-0.8.1 autograd-gamma-0.5.0 formulaic-0.6.2 interface-meta-1.3.0 lifelines-0.27.7 ngboost-0.4.1\n"
          ]
        }
      ],
      "source": [
        "# !rm -rf maril\n",
        "!git clone https://github.com/danishgufran/private.git\n",
        "!git clone --depth=1 https://danishgufran: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@github.com/saitiku/maril.git\n",
        "!pip install tensorflow-addons\n",
        "!pip install keras-multi-head\n",
        "!pip install catboost\n",
        "!pip install ngboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# from art.attacks.poisoning import PoisoningAttackBackdoor\n",
        "# from art.utils import load_dataset\n",
        "# from art.estimators.classification.scikitlearn import SklearnClassifier\n",
        "\n",
        "import copy\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D , LSTM, Attention\n",
        "from tensorflow.keras.losses import *\n",
        "from tensorflow.keras.optimizers import*\n",
        "import random as random\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "import pandas as pd\n",
        "\n",
        "import private.Stone_Seth.Seth\n",
        "from private.Stone_Seth.Seth import fetch_seth, Devices, Floorplan, get_mac_ids\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from maril.data import Devices, Floorplan, build_dataset\n",
        "from maril.helpers import compute_distances\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "\n",
        "def train_data(itr,dev, floorplan):\n",
        "    # dfs is a list of dataframes\n",
        "# meta is a dataframe with meta data\n",
        "\n",
        "#getting train data\n",
        "\n",
        "    train_fp, train_meta = fetch_seth(\n",
        "    dev,\n",
        "    str(floorplan),\n",
        "    ci = int(itr),\n",
        "    base_path=\"private/Stone_Seth/temp/clean/\"  # <-- this would be 'seth/temp/clean' from outside this dir\n",
        ")\n",
        "    # train_fp, _, macs, lbl2cord = build_dataset(\n",
        "    #     dev,\n",
        "    #     str(floorplan),\n",
        "    # )\n",
        "    train_fp = train_fp.sample(frac=1).reset_index(drop=True)\n",
        "    train_aps = get_mac_ids(train_fp.columns)\n",
        "    train_x = train_fp[train_aps].values\n",
        "    # train_x = (train_x + 100)/100\n",
        "    train_y = (train_fp[\"label\"]).values\n",
        "    return train_x, train_y, train_aps\n",
        "\n",
        "def test_data(itr, train_aps, dev, floorplan):\n",
        "    #getting test data\n",
        "    test_fp, test_meta = fetch_seth(\n",
        "    dev ,\n",
        "    str(floorplan),\n",
        "    ci = itr,\n",
        "    base_path=\"private/Stone_Seth/temp/clean/\"  # <-- this would be 'seth/temp/clean' from outside this dir\n",
        ")\n",
        "    # train_df, test_fp, macs_test, lbl2cords = build_dataset(\n",
        "    #       dev,\n",
        "    #       str(floorplan)\n",
        "    #   )\n",
        "    test_y = test_fp[\"label\"].values\n",
        "    # train_aps = train_aps.drop(['x', 'y','label'], axis=1)\n",
        "    # print(f'label -- {test_y}')\n",
        "    test_aps = get_mac_ids(test_fp.columns)\n",
        "    missing_aps = list(set(train_aps.columns)-set(test_aps))\n",
        "    test_fp[missing_aps] = 0\n",
        "\n",
        "    test_fp = test_fp.drop(['x', 'y','label'], axis=1)\n",
        "    test_x = test_fp[:]\n",
        "\n",
        "    # test_x = (test_x + 100)/100\n",
        "\n",
        "\n",
        "    return test_x, test_y\n",
        "\n",
        "def temp_train_data(dev, floorplan, ci_val):\n",
        "    # dfs is a list of dataframes\n",
        "# meta is a dataframe with meta data\n",
        "\n",
        "#getting train data\n",
        "\n",
        "    train_fp, train_meta = fetch_seth(\n",
        "    dev,\n",
        "    str(floorplan),\n",
        "    ci = ci_val,\n",
        "    base_path=\"private/Stone_Seth/temp/clean/\"  # <-- this would be 'seth/temp/clean' from outside this dir\n",
        ")\n",
        "    # train_fp, _, macs, lbl2cord = build_dataset(\n",
        "    #     dev,\n",
        "    #     str(floorplan),\n",
        "    # )\n",
        "    train_fp = train_fp.sample(frac=1).reset_index(drop=True)\n",
        "    train_aps = get_mac_ids(train_fp.columns)\n",
        "    train_x = train_fp[train_aps].values\n",
        "    train_x = (train_x + 100)/100\n",
        "    train_y = (train_fp[\"label\"]).values\n",
        "    return train_x, train_y, train_aps\n",
        "def temp_test_data(train_aps, dev, floorplan, ci_val):\n",
        "    #getting test data\n",
        "    test_fp, test_meta = fetch_seth(\n",
        "    str(dev) ,\n",
        "    str(floorplan),\n",
        "    ci = ci_val,\n",
        "    base_path=\"private/Stone_Seth/temp/clean/\"  # <-- this would be 'seth/temp/clean' from outside this dir\n",
        ")\n",
        "    # train_df, test_fp, macs_test, lbl2cords = build_dataset(\n",
        "    #       dev,\n",
        "    #       str(floorplan)\n",
        "    #   )\n",
        "    test_aps = get_mac_ids(test_fp.columns)\n",
        "    missing_aps = list(set(train_aps)-set(test_aps))\n",
        "    test_fp[missing_aps] = 0\n",
        "    test_x = test_fp[train_aps].values\n",
        "    test_x = (test_x + 100)/100\n",
        "    test_y = (test_fp[\"label\"]).values\n",
        "    return test_x, test_y"
      ],
      "metadata": {
        "id": "fZCKI2YL_5Pk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import LayerNormalization\n",
        "\n",
        "\n",
        "try:\n",
        "    from helpers import split_frame, compute_distances\n",
        "    from data import build_dataset\n",
        "    from Maril.Maril import (\n",
        "        MaskedDropout,\n",
        "        MaskedRandomContrast,\n",
        "        MaskedRandomBrightness,\n",
        "    )\n",
        "    from Maril.MultiHeadAttentionAddon import MultiHeadAttentionAddon\n",
        "except:\n",
        "    from maril.helpers import split_frame, compute_distances\n",
        "    from maril.data import build_dataset\n",
        "    from maril.Maril.Maril import (\n",
        "        MaskedDropout,\n",
        "        MaskedRandomContrast,\n",
        "        MaskedRandomBrightness,\n",
        "    )\n",
        "    from maril.Maril.MultiHeadAttentionAddon import MultiHeadAttentionAddon\n",
        "\n",
        "\n",
        "class Anvil:\n",
        "    \"\"\"\n",
        "    Manage and build model\n",
        "    NOTE: Lacks configurability; Needs fixing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_device,\n",
        "        floorplan,\n",
        "        num_heads=7,\n",
        "        head_size=50,\n",
        "        data_path=\"maril/Data/\",\n",
        "        model_path=\"Maril/saved_models\",\n",
        "        model_name=None,\n",
        "    ):\n",
        "        self.device = train_device\n",
        "        self.floorplan = floorplan\n",
        "        self.data_path = data_path\n",
        "        self.model_path = model_path\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = head_size\n",
        "        self.final = []\n",
        "\n",
        "\n",
        "        self.model = None\n",
        "\n",
        "        _, _, self.macs, _ = build_dataset(\n",
        "            self.device,\n",
        "            self.floorplan,\n",
        "            base_path=self.data_path,\n",
        "        )\n",
        "\n",
        "        # build meta\n",
        "        self.meta = {\n",
        "            \"NUM_HEADS\": num_heads,\n",
        "            \"HEAD_SIZE\": head_size,\n",
        "            \"MACS\": list(self.macs),\n",
        "            \"TRAIN_DEVICE\": self.device,\n",
        "            \"TRAIN_FLOORPLAN\": self.floorplan,\n",
        "        }\n",
        "\n",
        "        if model_name is None:\n",
        "            self.model_name = f\"DA_{train_device}_{floorplan}\"\n",
        "        else:\n",
        "            self.model_name = model_name\n",
        "\n",
        "    def build(self):\n",
        "\n",
        "        train_df, _, macs, lbl2cords = build_dataset(\n",
        "            self.device,\n",
        "            self.floorplan,\n",
        "            base_path=self.data_path,\n",
        "        )\n",
        "\n",
        "        td = 'LG'\n",
        "        train_df_rst, _, train_macs_rst, lbl2cord_rst = build_dataset(\n",
        "          td,\n",
        "          self.floorplan,\n",
        "      )\n",
        "        missing_waps_rst = list(set(macs) - set(train_macs_rst))\n",
        "        _df = train_df_rst.copy()  # supresses fragmented df warning\n",
        "        _df[missing_waps_rst] = 0.0\n",
        "        keys = _df[train_macs_rst].values.astype(float)\n",
        "        values = keras.utils.to_categorical(_df[\"label\"].values.astype(int))\n",
        "\n",
        "        if self.floorplan == 'engr0':\n",
        "          shp = 206\n",
        "        if self.floorplan == 'engr1':\n",
        "          shp = 203\n",
        "        if self.floorplan == 'glover':\n",
        "            shp = 78\n",
        "        if self.floorplan == 'sciences':\n",
        "            shp = 172\n",
        "        if self.floorplan == 'libstudy':\n",
        "            shp = 319\n",
        "\n",
        "        input_shape = shp\n",
        "        output_shape = len(lbl2cords.keys())\n",
        "        # keys = train_df[macs].values.astype(float)\n",
        "        # values = keras.utils.to_categorical(train_df[\"label\"].values.astype(int))\n",
        "\n",
        "        # input\n",
        "        input_layer = tf.keras.Input(shape=input_shape, name=\"query\")\n",
        "        x = input_layer\n",
        "        print(input_shape)\n",
        "\n",
        "        # augmentation # FASt Layer?\n",
        "        if \"NODA\" in self.model_name:\n",
        "            pass\n",
        "        else:\n",
        "            x = keras.layers.Normalization()(x)\n",
        "            x = keras.layers.Dropout(0.1)(x)\n",
        "\n",
        "\n",
        "            # x = MaskedDropout(0.10)(x)\n",
        "            # x = MaskedRandomBrightness(0.10, is_img=False)(x)\n",
        "            # x = MaskedRandomContrast(0.10, is_img=False)(x)\n",
        "\n",
        "        # noise\n",
        "        x = keras.layers.GaussianNoise(0.12)(x)\n",
        "\n",
        "        # MultiHeadLayer\n",
        "        x = MultiHeadAttentionAddon(\n",
        "            head_size=self.head_size,\n",
        "            num_heads=self.num_heads,\n",
        "            # output_size=None,\n",
        "            name=\"MHA\",\n",
        "            dropout=0.10,\n",
        "        )([x, keys, values])\n",
        "\n",
        "        # DNN layers\n",
        "        x = keras.layers.Dense(50, activation=\"relu\")(x)\n",
        "        x = keras.layers.Dropout(0.10)(x)\n",
        "        x = keras.layers.Dense(100, activation=\"relu\")(x)\n",
        "        x = keras.layers.Dropout(0.10)(x)\n",
        "\n",
        "        # output layer\n",
        "        output_layer = keras.layers.Dense(output_shape, activation=\"softmax\")(x)\n",
        "\n",
        "        # Connect the input and output model\n",
        "        self.model = tf.keras.Model(\n",
        "            inputs=input_layer, outputs=output_layer, name=self.model_name\n",
        "        )\n",
        "        return self.model\n",
        "\n",
        "    def create_siamese_network(self, input_shape, num_heads, key_dim, dff, embedding_dim):\n",
        "        output_classes = 61\n",
        "\n",
        "        input_anchor = Input(shape=input_shape, name='anchor_input')\n",
        "        input_positive = Input(shape=input_shape, name='positive_input')\n",
        "        input_negative = Input(shape=input_shape, name='negative_input')\n",
        "\n",
        "        shared_embedding = Dense(dff)\n",
        "\n",
        "        multihead_attention = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
        "        attention_layer_norm = LayerNormalization()\n",
        "        flatten = Flatten()\n",
        "\n",
        "        embedded_anchor = shared_embedding(input_anchor)\n",
        "        embedded_positive = shared_embedding(input_positive)\n",
        "        embedded_negative = shared_embedding(input_negative)\n",
        "\n",
        "        # One-hot encode train_y\n",
        "        train_y_input_tensor = Input(shape=(output_classes,), name='train_y_input')\n",
        "\n",
        "        attention_anchor = self.model(input_anchor)\n",
        "        attention_positive = self.model(input_positive)\n",
        "        attention_negative = self.model(input_negative)\n",
        "\n",
        "        attention_anchor = attention_layer_norm(attention_anchor)\n",
        "        attention_positive = attention_layer_norm(attention_positive)\n",
        "        attention_negative = attention_layer_norm(attention_negative)\n",
        "\n",
        "        embedded_anchor = Dense(embedding_dim, activation='relu')(flatten(attention_anchor))\n",
        "        embedded_positive = Dense(embedding_dim, activation='relu')(flatten(attention_positive))\n",
        "        embedded_negative = Dense(embedding_dim, activation='relu')(flatten(attention_negative))\n",
        "\n",
        "        # Use shape of train_y in Softmax layer\n",
        "        softmax_output = Dense(output_classes, activation='softmax', name='softmax_output')(embedded_anchor)\n",
        "\n",
        "        siamese_model = Model(\n",
        "            inputs=[input_anchor, input_positive, input_negative, train_y_input_tensor],\n",
        "            outputs=[embedded_anchor, embedded_positive, embedded_negative, softmax_output]\n",
        "        )\n",
        "        return siamese_model\n",
        "\n",
        "# This is just to test\n",
        "def shuffle_arrays(data_array):\n",
        "    np.random.shuffle(data_array)\n",
        "    return data_array\n",
        "\n",
        "# This is just to test\n",
        "def reverse_arrays(data_array):\n",
        "    reversed_data_array = np.flip(data_array, axis=0)\n",
        "    return reversed_data_array\n",
        "\n",
        "\n",
        "def replace_with_zeros(array, replace_percent):\n",
        "    num_samples = array.shape[0]\n",
        "    num_features = array.shape[1]\n",
        "    num_replacements = int(num_samples * num_features * replace_percent)\n",
        "\n",
        "    # Flatten the array\n",
        "    flattened_array = array.flatten()\n",
        "\n",
        "    # Get random indices for replacements\n",
        "    replace_indices = np.random.choice(num_samples * num_features, num_replacements, replace=True)\n",
        "\n",
        "    # Replace selected indices with zeros\n",
        "    flattened_array[replace_indices] = 0\n",
        "\n",
        "    # Reshape back to original shape\n",
        "    modified_array = flattened_array.reshape((num_samples, num_features))\n",
        "\n",
        "    return modified_array\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import Loss\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn import svm\n",
        "\n",
        "class TripletLoss(Loss):\n",
        "    def call(self, y_true, y_pred):\n",
        "        anchor, positive, negative = y_pred[:, 0], y_pred[:, 1], y_pred[:, 2]\n",
        "        distance_positive = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
        "        distance_negative = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
        "        return tf.maximum(distance_positive - distance_negative + 0.2, 0.0)\n",
        "        # return tf.maximum(distance_positive - distance_negative , 0.0)\n",
        "\n",
        "train_dev = ['LG']\n",
        "dev = ['BLU','HTC','LG','MOTO','OP3','S7']\n",
        "floorplan = ['engr1']\n",
        "\n",
        "for train in train_dev:\n",
        "    for flp in floorplan:\n",
        "        print(f'Train dev -> {train}   flp -> {flp}')\n",
        "        ci_val = 0\n",
        "        train_x, train_y, train_aps = temp_train_data(train, flp, ci_val)\n",
        "\n",
        "        # Example usage\n",
        "        input_shape = train_x.shape[1:]  # Exclude batch size\n",
        "        num_heads = 5\n",
        "        key_dim = 64\n",
        "        dff = 128\n",
        "        embedding_dim = 64\n",
        "        output_classes = train_y.shape[-1]  # Number of output classes\n",
        "\n",
        "        anvil_instance = Anvil(train, flp)\n",
        "        model = anvil_instance.build()  # Build the Anvil model\n",
        "        siamese_model = anvil_instance.create_siamese_network(\n",
        "            input_shape=input_shape,\n",
        "            num_heads=num_heads,\n",
        "            key_dim=key_dim,\n",
        "            dff=dff,\n",
        "            embedding_dim=embedding_dim\n",
        "        )\n",
        "        siamese_model.summary()\n",
        "\n",
        "        # Compile the model with triplet loss\n",
        "        siamese_model.compile(optimizer=Adam(learning_rate=0.00001), loss=TripletLoss())\n",
        "\n",
        "        # Drop D% of APs Randomly\n",
        "        train_x_modified = replace_with_zeros(train_x, 0.20)\n",
        "\n",
        "        # Example usage\n",
        "        # shuffled_train_x = shuffle_arrays(train_x)\n",
        "        shuffled_train_x = reverse_arrays(train_x)\n",
        "\n",
        "        # Train the model with your data\n",
        "        siamese_model.fit(\n",
        "            x=[train_x, train_x_modified, shuffled_train_x, train_y],\n",
        "            y=[train_y, train_y, train_y, train_y],\n",
        "            batch_size=64,\n",
        "            epochs=100\n",
        "        )\n",
        "\n",
        "        # Extract the encoded output for training data\n",
        "        anchor_siamese_model = Model(siamese_model.input[0], siamese_model.output[-1])\n",
        "        encoded_train_output = anchor_siamese_model.predict(train_x)\n",
        "        # encoded_train_output_reshaped = []\n",
        "\n",
        "        # # Reshape each output in the list\n",
        "        # for output in encoded_train_output:\n",
        "        #     reshaped_output = output.reshape((output.shape[0], -1))\n",
        "        #     encoded_train_output_reshaped.append(reshaped_output)\n",
        "\n",
        "        # # Concatenate the reshaped outputs\n",
        "        # concatenated_output = np.concatenate(encoded_train_output_reshaped, axis=1)\n",
        "\n",
        "        # Create and fit the KNN classifier\n",
        "        knn_classifier = KNeighborsClassifier(n_neighbors=4)\n",
        "        print('Training the KNN classifier')\n",
        "        knn_classifier.fit(encoded_train_output, train_y)\n",
        "\n",
        "        # Create and fit the Random Forest classifier\n",
        "        random_forest_classifier = RandomForestClassifier(n_estimators=100)\n",
        "        print('Training the RF classifier')\n",
        "        random_forest_classifier.fit(encoded_train_output, train_y)\n",
        "\n",
        "        # Create and fit the SVM classifier\n",
        "        svm_classifier = svm.SVC(kernel='rbf')\n",
        "        print('Training the SVM classifier')\n",
        "        svm_classifier.fit(encoded_train_output, train_y)\n",
        "\n",
        "        import xgboost as xgb\n",
        "\n",
        "        # Set the parameters for XGBoost\n",
        "        params = {\n",
        "            'objective': 'multi:softmax',\n",
        "            'num_class': len(np.unique(train_y)),\n",
        "            'max_depth': 3,\n",
        "            'eta': 0.1,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8\n",
        "        }\n",
        "\n",
        "\n",
        "        # Create the XGBoost classifier\n",
        "        print('Training the XgBoost classifier')\n",
        "        xgb_classifier = xgb.XGBClassifier(**params)\n",
        "\n",
        "        # Train the XGBoost classifier\n",
        "        xgb_classifier.fit(encoded_train_output, train_y)\n",
        "\n",
        "        from catboost import CatBoostClassifier\n",
        "        # Set the parameters for CatBoost\n",
        "        params = {\n",
        "            'iterations': 35,\n",
        "            'learning_rate': 0.1,\n",
        "            'depth': 6,\n",
        "            'loss_function': 'MultiClass',\n",
        "            'custom_metric': 'Accuracy',\n",
        "            'random_seed': 42\n",
        "        }\n",
        "\n",
        "        # Create the CatBoost classifier\n",
        "        print('Training the CatBoost classifier')\n",
        "        catboost_classifier = CatBoostClassifier(**params)\n",
        "\n",
        "        # Train the CatBoost classifier\n",
        "        catboost_classifier.fit(encoded_train_output, train_y)\n",
        "\n",
        "        import ngboost as ngb\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "        from ngboost import NGBRegressor\n",
        "\n",
        "        ngb_classifier = NGBRegressor().fit(encoded_train_output, train_y)\n",
        "        # Y_preds = ngb.predict(encoded_train_output)\n",
        "\n",
        "        # import lightgbm as lgb\n",
        "        # # Set the parameters for LightGBM\n",
        "        # params = {\n",
        "        #     'objective': 'multiclass',\n",
        "        #     'num_class': len(np.unique(train_y)),\n",
        "        #     'metric': 'multi_logloss',\n",
        "        #     'num_leaves': 31,\n",
        "        #     'learning_rate': 0.1,\n",
        "        #     'feature_fraction': 0.9,\n",
        "        #     'bagging_fraction': 0.8,\n",
        "        #     'bagging_freq': 5,\n",
        "        #     'verbose': -1,\n",
        "        #     'random_state': 42\n",
        "        # }\n",
        "\n",
        "        # # Train the LightGBM classifier\n",
        "        # print('Training the LightGBM classifier')\n",
        "        # lightgbm_classifier = lgb.train(params, encoded_train_output, num_boost_round=100)\n",
        "\n",
        "\n",
        "\n",
        "        # for test_dev in dev:\n",
        "        #     ci_val = 0\n",
        "        #     print(f'Test dev -> {test_dev}   flp -> {floorplan[0]}  Ci -> {ci_val}')\n",
        "        #     # test_x, test_y = temp_test_data(train_aps, dev, floorplan[0], ci_val)\n",
        "        #     prediction_model = Model(siamese_model.input[0], siamese_model.output[-1])\n",
        "\n",
        "        #     pred = prediction_model.predict(train_x)\n",
        "        #     # encoded_test_output_reshaped = []\n",
        "\n",
        "        #     # # Reshape each output in the list\n",
        "        #     # for output in pred:\n",
        "        #     #     reshaped_output = output.reshape((output.shape[0], -1))\n",
        "        #     #     encoded_test_output_reshaped.append(reshaped_output)\n",
        "\n",
        "        #     # # Concatenate the reshaped outputs\n",
        "        #     # concatenated_output_test = np.concatenate(encoded_test_output_reshaped, axis=1)\n",
        "        #     predicted_labels = knn_classifier.predict(pred)\n",
        "        #     # pred = siamese_model.predict([train_x, train_x, train_x, train_y])\n",
        "\n",
        "        #     # mean_error = np.mean(compute_distances(pred_y, train_y))\n",
        "        #     print(predicted_labels)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4JeoWHnJ-Jp",
        "outputId": "c5506233-306a-4c10-ee08-c4c6d57b0942"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dev -> LG   flp -> engr1\n",
            "203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/initializers/initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_246\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " anchor_input (InputLayer)      [(None, 203)]        0           []                               \n",
            "                                                                                                  \n",
            " DA_LG_engr1 (Functional)       (None, 48)           188203      ['anchor_input[0][0]',           \n",
            "                                                                  'positive_input[0][0]',         \n",
            "                                                                  'negative_input[0][0]']         \n",
            "                                                                                                  \n",
            " positive_input (InputLayer)    [(None, 203)]        0           []                               \n",
            "                                                                                                  \n",
            " negative_input (InputLayer)    [(None, 203)]        0           []                               \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 48)          96          ['DA_LG_engr1[0][0]',            \n",
            " rmalization)                                                     'DA_LG_engr1[1][0]',            \n",
            "                                                                  'DA_LG_engr1[2][0]']            \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 48)           0           ['layer_normalization_3[0][0]',  \n",
            "                                                                  'layer_normalization_3[1][0]',  \n",
            "                                                                  'layer_normalization_3[2][0]']  \n",
            "                                                                                                  \n",
            " dense_25 (Dense)               (None, 64)           3136        ['flatten_3[0][0]']              \n",
            "                                                                                                  \n",
            " train_y_input (InputLayer)     [(None, 61)]         0           []                               \n",
            "                                                                                                  \n",
            " dense_26 (Dense)               (None, 64)           3136        ['flatten_3[1][0]']              \n",
            "                                                                                                  \n",
            " dense_27 (Dense)               (None, 64)           3136        ['flatten_3[2][0]']              \n",
            "                                                                                                  \n",
            " softmax_output (Dense)         (None, 61)           3965        ['dense_25[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 201,672\n",
            "Trainable params: 201,265\n",
            "Non-trainable params: 407\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "5/5 [==============================] - 6s 45ms/step - loss: 0.8003 - dense_25_loss: 0.2001 - dense_26_loss: 0.2000 - dense_27_loss: 0.2002 - softmax_output_loss: 0.2000\n",
            "Epoch 2/100\n",
            "5/5 [==============================] - 0s 44ms/step - loss: 0.8001 - dense_25_loss: 0.2000 - dense_26_loss: 0.2000 - dense_27_loss: 0.2001 - softmax_output_loss: 0.2000\n",
            "Epoch 3/100\n",
            "5/5 [==============================] - 0s 42ms/step - loss: 0.8000 - dense_25_loss: 0.2000 - dense_26_loss: 0.2000 - dense_27_loss: 0.2000 - softmax_output_loss: 0.2000\n",
            "Epoch 4/100\n",
            "5/5 [==============================] - 0s 42ms/step - loss: 0.8000 - dense_25_loss: 0.2000 - dense_26_loss: 0.2000 - dense_27_loss: 0.2000 - softmax_output_loss: 0.2000\n",
            "Epoch 5/100\n",
            "5/5 [==============================] - 0s 44ms/step - loss: 0.7999 - dense_25_loss: 0.2000 - dense_26_loss: 0.2000 - dense_27_loss: 0.2000 - softmax_output_loss: 0.2000\n",
            "Epoch 6/100\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.7999 - dense_25_loss: 0.2000 - dense_26_loss: 0.2000 - dense_27_loss: 0.1999 - softmax_output_loss: 0.2000\n",
            "Epoch 7/100\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.7997 - dense_25_loss: 0.2000 - dense_26_loss: 0.1999 - dense_27_loss: 0.1998 - softmax_output_loss: 0.2000\n",
            "Epoch 8/100\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.7994 - dense_25_loss: 0.2000 - dense_26_loss: 0.1998 - dense_27_loss: 0.1997 - softmax_output_loss: 0.2000\n",
            "Epoch 9/100\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.7989 - dense_25_loss: 0.1999 - dense_26_loss: 0.1995 - dense_27_loss: 0.1995 - softmax_output_loss: 0.2000\n",
            "Epoch 10/100\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.7983 - dense_25_loss: 0.1999 - dense_26_loss: 0.1992 - dense_27_loss: 0.1992 - softmax_output_loss: 0.2000\n",
            "Epoch 11/100\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.7974 - dense_25_loss: 0.1998 - dense_26_loss: 0.1986 - dense_27_loss: 0.1990 - softmax_output_loss: 0.2000\n",
            "Epoch 12/100\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 0.7962 - dense_25_loss: 0.1997 - dense_26_loss: 0.1977 - dense_27_loss: 0.1988 - softmax_output_loss: 0.2000\n",
            "Epoch 13/100\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.7948 - dense_25_loss: 0.1995 - dense_26_loss: 0.1965 - dense_27_loss: 0.1988 - softmax_output_loss: 0.2000\n",
            "Epoch 14/100\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.7926 - dense_25_loss: 0.1991 - dense_26_loss: 0.1949 - dense_27_loss: 0.1986 - softmax_output_loss: 0.2000\n",
            "Epoch 15/100\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.7902 - dense_25_loss: 0.1986 - dense_26_loss: 0.1930 - dense_27_loss: 0.1986 - softmax_output_loss: 0.2000\n",
            "Epoch 16/100\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 0.7873 - dense_25_loss: 0.1978 - dense_26_loss: 0.1908 - dense_27_loss: 0.1986 - softmax_output_loss: 0.2000\n",
            "Epoch 17/100\n",
            "5/5 [==============================] - 0s 44ms/step - loss: 0.7835 - dense_25_loss: 0.1970 - dense_26_loss: 0.1878 - dense_27_loss: 0.1988 - softmax_output_loss: 0.2000\n",
            "Epoch 18/100\n",
            "5/5 [==============================] - 0s 47ms/step - loss: 0.7801 - dense_25_loss: 0.1961 - dense_26_loss: 0.1850 - dense_27_loss: 0.1990 - softmax_output_loss: 0.2000\n",
            "Epoch 19/100\n",
            "5/5 [==============================] - 0s 44ms/step - loss: 0.7748 - dense_25_loss: 0.1942 - dense_26_loss: 0.1815 - dense_27_loss: 0.1991 - softmax_output_loss: 0.2000\n",
            "Epoch 20/100\n",
            "5/5 [==============================] - 0s 42ms/step - loss: 0.7700 - dense_25_loss: 0.1926 - dense_26_loss: 0.1780 - dense_27_loss: 0.1993 - softmax_output_loss: 0.2000\n",
            "Epoch 21/100\n",
            "5/5 [==============================] - 0s 44ms/step - loss: 0.7636 - dense_25_loss: 0.1904 - dense_26_loss: 0.1737 - dense_27_loss: 0.1994 - softmax_output_loss: 0.2000\n",
            "Epoch 22/100\n",
            "5/5 [==============================] - 0s 43ms/step - loss: 0.7565 - dense_25_loss: 0.1884 - dense_26_loss: 0.1684 - dense_27_loss: 0.1997 - softmax_output_loss: 0.2000\n",
            "Epoch 23/100\n",
            "5/5 [==============================] - 0s 42ms/step - loss: 0.7498 - dense_25_loss: 0.1857 - dense_26_loss: 0.1641 - dense_27_loss: 0.2000 - softmax_output_loss: 0.2000\n",
            "Epoch 24/100\n",
            "5/5 [==============================] - 0s 42ms/step - loss: 0.7405 - dense_25_loss: 0.1826 - dense_26_loss: 0.1578 - dense_27_loss: 0.2000 - softmax_output_loss: 0.2000\n",
            "Epoch 25/100\n",
            "5/5 [==============================] - 0s 44ms/step - loss: 0.7314 - dense_25_loss: 0.1787 - dense_26_loss: 0.1526 - dense_27_loss: 0.2002 - softmax_output_loss: 0.2000\n",
            "Epoch 26/100\n",
            "5/5 [==============================] - 0s 44ms/step - loss: 0.7192 - dense_25_loss: 0.1733 - dense_26_loss: 0.1454 - dense_27_loss: 0.2005 - softmax_output_loss: 0.2000\n",
            "Epoch 27/100\n",
            "5/5 [==============================] - 0s 47ms/step - loss: 0.7093 - dense_25_loss: 0.1698 - dense_26_loss: 0.1390 - dense_27_loss: 0.2004 - softmax_output_loss: 0.2000\n",
            "Epoch 28/100\n",
            "5/5 [==============================] - 0s 43ms/step - loss: 0.6965 - dense_25_loss: 0.1638 - dense_26_loss: 0.1323 - dense_27_loss: 0.2004 - softmax_output_loss: 0.2000\n",
            "Epoch 29/100\n",
            "5/5 [==============================] - 0s 42ms/step - loss: 0.6806 - dense_25_loss: 0.1574 - dense_26_loss: 0.1229 - dense_27_loss: 0.2003 - softmax_output_loss: 0.2000\n",
            "Epoch 30/100\n",
            "5/5 [==============================] - 0s 42ms/step - loss: 0.6663 - dense_25_loss: 0.1518 - dense_26_loss: 0.1144 - dense_27_loss: 0.2001 - softmax_output_loss: 0.2000\n",
            "Epoch 31/100\n",
            "5/5 [==============================] - 0s 43ms/step - loss: 0.6504 - dense_25_loss: 0.1439 - dense_26_loss: 0.1068 - dense_27_loss: 0.1997 - softmax_output_loss: 0.2000\n",
            "Epoch 32/100\n",
            "5/5 [==============================] - 0s 43ms/step - loss: 0.6322 - dense_25_loss: 0.1375 - dense_26_loss: 0.0957 - dense_27_loss: 0.1991 - softmax_output_loss: 0.2000\n",
            "Epoch 33/100\n",
            "5/5 [==============================] - 0s 45ms/step - loss: 0.6136 - dense_25_loss: 0.1282 - dense_26_loss: 0.0870 - dense_27_loss: 0.1984 - softmax_output_loss: 0.2000\n",
            "Epoch 34/100\n",
            "5/5 [==============================] - 0s 40ms/step - loss: 0.5864 - dense_25_loss: 0.1161 - dense_26_loss: 0.0729 - dense_27_loss: 0.1974 - softmax_output_loss: 0.2000\n",
            "Epoch 35/100\n",
            "5/5 [==============================] - 0s 46ms/step - loss: 0.5680 - dense_25_loss: 0.1088 - dense_26_loss: 0.0629 - dense_27_loss: 0.1963 - softmax_output_loss: 0.2000\n",
            "Epoch 36/100\n",
            "5/5 [==============================] - 0s 45ms/step - loss: 0.5435 - dense_25_loss: 0.1003 - dense_26_loss: 0.0481 - dense_27_loss: 0.1951 - softmax_output_loss: 0.2000\n",
            "Epoch 37/100\n",
            "5/5 [==============================] - 0s 42ms/step - loss: 0.5115 - dense_25_loss: 0.0855 - dense_26_loss: 0.0328 - dense_27_loss: 0.1931 - softmax_output_loss: 0.2000\n",
            "Epoch 38/100\n",
            "5/5 [==============================] - 0s 43ms/step - loss: 0.4849 - dense_25_loss: 0.0737 - dense_26_loss: 0.0200 - dense_27_loss: 0.1913 - softmax_output_loss: 0.2000\n",
            "Epoch 39/100\n",
            "5/5 [==============================] - 0s 46ms/step - loss: 0.4600 - dense_25_loss: 0.0604 - dense_26_loss: 0.0103 - dense_27_loss: 0.1892 - softmax_output_loss: 0.2000\n",
            "Epoch 40/100\n",
            "5/5 [==============================] - 0s 45ms/step - loss: 0.4465 - dense_25_loss: 0.0503 - dense_26_loss: 0.0099 - dense_27_loss: 0.1864 - softmax_output_loss: 0.2000\n",
            "Epoch 41/100\n",
            "5/5 [==============================] - 0s 42ms/step - loss: 0.4250 - dense_25_loss: 0.0312 - dense_26_loss: 0.0101 - dense_27_loss: 0.1837 - softmax_output_loss: 0.2000\n",
            "Epoch 42/100\n",
            "5/5 [==============================] - 0s 46ms/step - loss: 0.4098 - dense_25_loss: 0.0189 - dense_26_loss: 0.0088 - dense_27_loss: 0.1821 - softmax_output_loss: 0.2000\n",
            "Epoch 43/100\n",
            "5/5 [==============================] - 0s 44ms/step - loss: 0.3977 - dense_25_loss: 0.0105 - dense_26_loss: 0.0083 - dense_27_loss: 0.1788 - softmax_output_loss: 0.2000\n",
            "Epoch 44/100\n",
            "5/5 [==============================] - 0s 46ms/step - loss: 0.3944 - dense_25_loss: 0.0095 - dense_26_loss: 0.0086 - dense_27_loss: 0.1762 - softmax_output_loss: 0.2000\n",
            "Epoch 45/100\n",
            "5/5 [==============================] - 0s 50ms/step - loss: 0.3904 - dense_25_loss: 0.0094 - dense_26_loss: 0.0073 - dense_27_loss: 0.1736 - softmax_output_loss: 0.2000   \n",
            "Epoch 46/100\n",
            "5/5 [==============================] - 0s 42ms/step - loss: 0.3861 - dense_25_loss: 0.0088 - dense_26_loss: 0.0077 - dense_27_loss: 0.1696 - softmax_output_loss: 0.2000\n",
            "Epoch 47/100\n",
            "5/5 [==============================] - 0s 43ms/step - loss: 0.3835 - dense_25_loss: 0.0081 - dense_26_loss: 0.0086 - dense_27_loss: 0.1668 - softmax_output_loss: 0.2000\n",
            "Epoch 48/100\n",
            "5/5 [==============================] - 0s 43ms/step - loss: 0.3797 - dense_25_loss: 0.0094 - dense_26_loss: 0.0071 - dense_27_loss: 0.1632 - softmax_output_loss: 0.2000\n",
            "Epoch 49/100\n",
            "5/5 [==============================] - 0s 44ms/step - loss: 0.3746 - dense_25_loss: 0.0075 - dense_26_loss: 0.0069 - dense_27_loss: 0.1603 - softmax_output_loss: 0.2000   \n",
            "Epoch 50/100\n",
            "5/5 [==============================] - 0s 43ms/step - loss: 0.3691 - dense_25_loss: 0.0076 - dense_26_loss: 0.0071 - dense_27_loss: 0.1545 - softmax_output_loss: 0.2000\n",
            "Epoch 51/100\n",
            "5/5 [==============================] - 0s 44ms/step - loss: 0.3635 - dense_25_loss: 0.0067 - dense_26_loss: 0.0058 - dense_27_loss: 0.1509 - softmax_output_loss: 0.2000\n",
            "Epoch 52/100\n",
            "5/5 [==============================] - 0s 43ms/step - loss: 0.3612 - dense_25_loss: 0.0081 - dense_26_loss: 0.0063 - dense_27_loss: 0.1469 - softmax_output_loss: 0.2000\n",
            "Epoch 53/100\n",
            "5/5 [==============================] - 0s 44ms/step - loss: 0.3565 - dense_25_loss: 0.0069 - dense_26_loss: 0.0057 - dense_27_loss: 0.1439 - softmax_output_loss: 0.2000   \n",
            "Epoch 54/100\n",
            "5/5 [==============================] - 0s 51ms/step - loss: 0.3487 - dense_25_loss: 0.0063 - dense_26_loss: 0.0058 - dense_27_loss: 0.1365 - softmax_output_loss: 0.2000   \n",
            "Epoch 55/100\n",
            "5/5 [==============================] - 0s 43ms/step - loss: 0.3450 - dense_25_loss: 0.0054 - dense_26_loss: 0.0064 - dense_27_loss: 0.1333 - softmax_output_loss: 0.2000   \n",
            "Epoch 56/100\n",
            "5/5 [==============================] - 0s 41ms/step - loss: 0.3378 - dense_25_loss: 0.0052 - dense_26_loss: 0.0055 - dense_27_loss: 0.1271 - softmax_output_loss: 0.2000\n",
            "Epoch 57/100\n",
            "5/5 [==============================] - 0s 43ms/step - loss: 0.3328 - dense_25_loss: 0.0057 - dense_26_loss: 0.0046 - dense_27_loss: 0.1226 - softmax_output_loss: 0.2000   \n",
            "Epoch 58/100\n",
            "5/5 [==============================] - 0s 49ms/step - loss: 0.3219 - dense_25_loss: 0.0053 - dense_26_loss: 0.0044 - dense_27_loss: 0.1122 - softmax_output_loss: 0.2000\n",
            "Epoch 59/100\n",
            "5/5 [==============================] - 0s 41ms/step - loss: 0.3122 - dense_25_loss: 0.0042 - dense_26_loss: 0.0038 - dense_27_loss: 0.1043 - softmax_output_loss: 0.2000\n",
            "Epoch 60/100\n",
            "5/5 [==============================] - 0s 57ms/step - loss: 0.3031 - dense_25_loss: 0.0030 - dense_26_loss: 0.0037 - dense_27_loss: 0.0964 - softmax_output_loss: 0.2000\n",
            "Epoch 61/100\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.2983 - dense_25_loss: 0.0045 - dense_26_loss: 0.0032 - dense_27_loss: 0.0906 - softmax_output_loss: 0.2000\n",
            "Epoch 62/100\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 0.2845 - dense_25_loss: 0.0022 - dense_26_loss: 0.0028 - dense_27_loss: 0.0796 - softmax_output_loss: 0.2000\n",
            "Epoch 63/100\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.2748 - dense_25_loss: 1.4196e-04 - dense_26_loss: 0.0013 - dense_27_loss: 0.0733 - softmax_output_loss: 0.2000\n",
            "Epoch 64/100\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.2602 - dense_25_loss: 3.7237e-04 - dense_26_loss: 0.0018 - dense_27_loss: 0.0580 - softmax_output_loss: 0.2000\n",
            "Epoch 65/100\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.2497 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0015 - dense_27_loss: 0.0482 - softmax_output_loss: 0.2000\n",
            "Epoch 66/100\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.2327 - dense_25_loss: 5.9152e-04 - dense_26_loss: 2.1900e-04 - dense_27_loss: 0.0319 - softmax_output_loss: 0.2000\n",
            "Epoch 67/100\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.2255 - dense_25_loss: 7.7280e-04 - dense_26_loss: 2.5715e-04 - dense_27_loss: 0.0245 - softmax_output_loss: 0.2000\n",
            "Epoch 68/100\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 0.2104 - dense_25_loss: 0.0000e+00 - dense_26_loss: 3.8902e-04 - dense_27_loss: 0.0100 - softmax_output_loss: 0.2000\n",
            "Epoch 69/100\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 0.2093 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0094 - softmax_output_loss: 0.2000\n",
            "Epoch 70/100\n",
            "5/5 [==============================] - 0s 79ms/step - loss: 0.2110 - dense_25_loss: 0.0000e+00 - dense_26_loss: 8.5197e-04 - dense_27_loss: 0.0102 - softmax_output_loss: 0.2000\n",
            "Epoch 71/100\n",
            "5/5 [==============================] - 0s 51ms/step - loss: 0.2101 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0101 - softmax_output_loss: 0.2000\n",
            "Epoch 72/100\n",
            "5/5 [==============================] - 0s 45ms/step - loss: 0.2099 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0099 - softmax_output_loss: 0.2000\n",
            "Epoch 73/100\n",
            "5/5 [==============================] - 0s 44ms/step - loss: 0.2087 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0087 - softmax_output_loss: 0.2000\n",
            "Epoch 74/100\n",
            "5/5 [==============================] - 0s 49ms/step - loss: 0.2091 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0091 - softmax_output_loss: 0.2000\n",
            "Epoch 75/100\n",
            "5/5 [==============================] - 0s 43ms/step - loss: 0.2093 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0093 - softmax_output_loss: 0.2000\n",
            "Epoch 76/100\n",
            "5/5 [==============================] - 0s 45ms/step - loss: 0.2072 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0072 - softmax_output_loss: 0.2000\n",
            "Epoch 77/100\n",
            "5/5 [==============================] - 0s 43ms/step - loss: 0.2089 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0089 - softmax_output_loss: 0.2000\n",
            "Epoch 78/100\n",
            "5/5 [==============================] - 0s 49ms/step - loss: 0.2093 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0093 - softmax_output_loss: 0.1999\n",
            "Epoch 79/100\n",
            "5/5 [==============================] - 0s 41ms/step - loss: 0.2077 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0078 - softmax_output_loss: 0.1999\n",
            "Epoch 80/100\n",
            "5/5 [==============================] - 0s 42ms/step - loss: 0.2088 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0089 - softmax_output_loss: 0.1999\n",
            "Epoch 81/100\n",
            "5/5 [==============================] - 0s 44ms/step - loss: 0.2087 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0088 - softmax_output_loss: 0.1999\n",
            "Epoch 82/100\n",
            "5/5 [==============================] - 0s 41ms/step - loss: 0.2085 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0086 - softmax_output_loss: 0.1999\n",
            "Epoch 83/100\n",
            "5/5 [==============================] - 0s 46ms/step - loss: 0.2068 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0069 - softmax_output_loss: 0.1999\n",
            "Epoch 84/100\n",
            "5/5 [==============================] - 0s 44ms/step - loss: 0.2080 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0080 - softmax_output_loss: 0.1999\n",
            "Epoch 85/100\n",
            "5/5 [==============================] - 0s 42ms/step - loss: 0.2081 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0081 - softmax_output_loss: 0.1999\n",
            "Epoch 86/100\n",
            "5/5 [==============================] - 0s 46ms/step - loss: 0.2074 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0075 - softmax_output_loss: 0.1999\n",
            "Epoch 87/100\n",
            "5/5 [==============================] - 0s 49ms/step - loss: 0.2078 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0079 - softmax_output_loss: 0.1999\n",
            "Epoch 88/100\n",
            "5/5 [==============================] - 0s 42ms/step - loss: 0.2067 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0068 - softmax_output_loss: 0.1999\n",
            "Epoch 89/100\n",
            "5/5 [==============================] - 0s 44ms/step - loss: 0.2063 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0064 - softmax_output_loss: 0.1999\n",
            "Epoch 90/100\n",
            "5/5 [==============================] - 0s 44ms/step - loss: 0.2062 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0063 - softmax_output_loss: 0.1999\n",
            "Epoch 91/100\n",
            "5/5 [==============================] - 0s 47ms/step - loss: 0.2072 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0073 - softmax_output_loss: 0.1999\n",
            "Epoch 92/100\n",
            "5/5 [==============================] - 0s 46ms/step - loss: 0.2063 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0064 - softmax_output_loss: 0.1999\n",
            "Epoch 93/100\n",
            "5/5 [==============================] - 0s 43ms/step - loss: 0.2067 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0068 - softmax_output_loss: 0.1999\n",
            "Epoch 94/100\n",
            "5/5 [==============================] - 0s 41ms/step - loss: 0.2073 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0075 - softmax_output_loss: 0.1999\n",
            "Epoch 95/100\n",
            "5/5 [==============================] - 0s 45ms/step - loss: 0.2060 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0062 - softmax_output_loss: 0.1999\n",
            "Epoch 96/100\n",
            "5/5 [==============================] - 0s 48ms/step - loss: 0.2064 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0065 - softmax_output_loss: 0.1999\n",
            "Epoch 97/100\n",
            "5/5 [==============================] - 0s 45ms/step - loss: 0.2066 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0067 - softmax_output_loss: 0.1999\n",
            "Epoch 98/100\n",
            "5/5 [==============================] - 0s 46ms/step - loss: 0.2059 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0060 - softmax_output_loss: 0.1999\n",
            "Epoch 99/100\n",
            "5/5 [==============================] - 0s 44ms/step - loss: 0.2048 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0050 - softmax_output_loss: 0.1999\n",
            "Epoch 100/100\n",
            "5/5 [==============================] - 0s 44ms/step - loss: 0.2055 - dense_25_loss: 0.0000e+00 - dense_26_loss: 0.0000e+00 - dense_27_loss: 0.0056 - softmax_output_loss: 0.1999\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Training the KNN classifier\n",
            "Training the RF classifier\n",
            "Training the SVM classifier\n",
            "Training the XgBoost classifier\n",
            "Training the CatBoost classifier\n",
            "0:\tlearn: 3.7963398\ttotal: 327ms\tremaining: 11.1s\n",
            "1:\tlearn: 3.7192861\ttotal: 661ms\tremaining: 10.9s\n",
            "2:\tlearn: 3.6592982\ttotal: 1.01s\tremaining: 10.7s\n",
            "3:\tlearn: 3.5827678\ttotal: 1.34s\tremaining: 10.4s\n",
            "4:\tlearn: 3.5145410\ttotal: 1.67s\tremaining: 10s\n",
            "5:\tlearn: 3.4451464\ttotal: 2.01s\tremaining: 9.72s\n",
            "6:\tlearn: 3.3795445\ttotal: 2.34s\tremaining: 9.35s\n",
            "7:\tlearn: 3.3208146\ttotal: 2.67s\tremaining: 9s\n",
            "8:\tlearn: 3.2644235\ttotal: 3.02s\tremaining: 8.73s\n",
            "9:\tlearn: 3.2093344\ttotal: 3.34s\tremaining: 8.36s\n",
            "10:\tlearn: 3.1544000\ttotal: 3.68s\tremaining: 8.03s\n",
            "11:\tlearn: 3.0975674\ttotal: 4.01s\tremaining: 7.68s\n",
            "12:\tlearn: 3.0442441\ttotal: 4.33s\tremaining: 7.33s\n",
            "13:\tlearn: 2.9995129\ttotal: 4.67s\tremaining: 7s\n",
            "14:\tlearn: 2.9456148\ttotal: 5.01s\tremaining: 6.68s\n",
            "15:\tlearn: 2.9000176\ttotal: 5.33s\tremaining: 6.33s\n",
            "16:\tlearn: 2.8547523\ttotal: 5.68s\tremaining: 6.01s\n",
            "17:\tlearn: 2.8089504\ttotal: 6.01s\tremaining: 5.67s\n",
            "18:\tlearn: 2.7603566\ttotal: 6.33s\tremaining: 5.33s\n",
            "19:\tlearn: 2.7137351\ttotal: 6.65s\tremaining: 4.99s\n",
            "20:\tlearn: 2.6705935\ttotal: 7s\tremaining: 4.67s\n",
            "21:\tlearn: 2.6278626\ttotal: 7.34s\tremaining: 4.33s\n",
            "22:\tlearn: 2.5936968\ttotal: 7.7s\tremaining: 4.02s\n",
            "23:\tlearn: 2.5541872\ttotal: 8.3s\tremaining: 3.81s\n",
            "24:\tlearn: 2.5209706\ttotal: 8.93s\tremaining: 3.57s\n",
            "25:\tlearn: 2.4954493\ttotal: 9.54s\tremaining: 3.3s\n",
            "26:\tlearn: 2.4549895\ttotal: 10.1s\tremaining: 3s\n",
            "27:\tlearn: 2.4187750\ttotal: 10.7s\tremaining: 2.68s\n",
            "28:\tlearn: 2.3848252\ttotal: 11.1s\tremaining: 2.29s\n",
            "29:\tlearn: 2.3548609\ttotal: 11.4s\tremaining: 1.9s\n",
            "30:\tlearn: 2.3213836\ttotal: 11.7s\tremaining: 1.51s\n",
            "31:\tlearn: 2.2845235\ttotal: 12.1s\tremaining: 1.13s\n",
            "32:\tlearn: 2.2536871\ttotal: 12.4s\tremaining: 751ms\n",
            "33:\tlearn: 2.2203054\ttotal: 12.7s\tremaining: 374ms\n",
            "34:\tlearn: 2.1901084\ttotal: 13.1s\tremaining: 0us\n",
            "[iter 0] loss=4.0475 val_loss=0.0000 scale=1.0000 norm=12.0210\n",
            "[iter 100] loss=3.8714 val_loss=0.0000 scale=2.0000 norm=20.3903\n",
            "[iter 200] loss=3.7696 val_loss=0.0000 scale=2.0000 norm=19.9884\n",
            "[iter 300] loss=3.7078 val_loss=0.0000 scale=2.0000 norm=19.8868\n",
            "[iter 400] loss=3.6652 val_loss=0.0000 scale=4.0000 norm=39.7342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "floorplan = ['engr0', 'engr1', 'glover', 'sciences', 'libstudy']\n",
        "final_knn = []\n",
        "final_rf = []\n",
        "final_svm = []\n",
        "final_xgb = []\n",
        "final_ctb = []\n",
        "final_ngb = []\n",
        "for ci_val in range(0, 10):\n",
        "  for test_dev in dev:\n",
        "      # ci_val = 6\n",
        "      num = 1\n",
        "      print(f'Test dev -> {test_dev}   flp -> {floorplan[num]}  Ci -> {ci_val}')\n",
        "      test_x, test_y = temp_test_data(train_aps, test_dev, floorplan[num], ci_val)\n",
        "      prediction_model = Model(siamese_model.input[0], siamese_model.output[-1])\n",
        "\n",
        "      pred = prediction_model.predict(test_x)\n",
        "      # encoded_test_output_reshaped = []\n",
        "\n",
        "      # # Reshape each output in the list\n",
        "      # for output in pred:\n",
        "      #     reshaped_output = output.reshape((output.shape[0], -1))\n",
        "      #     encoded_test_output_reshaped.append(reshaped_output)\n",
        "\n",
        "      # # Concatenate the reshaped outputs\n",
        "      # concatenated_output_test = np.concatenate(encoded_test_output_reshaped, axis=1)\n",
        "      predicted_labels_knn = knn_classifier.predict(pred)\n",
        "      predicted_labels_rf = random_forest_classifier.predict(pred)\n",
        "      predicted_labels_svm = svm_classifier.predict(pred)\n",
        "      predicted_labels_xgb = xgb_classifier.predict(pred)\n",
        "      predicted_labels_ctb = catboost_classifier.predict(pred)\n",
        "      predicted_labels_ngb = ngb_classifier.predict(pred)\n",
        "      # predicted_labels_lgbm = lightgbm_classifier.predict(pred)\n",
        "      # pred = siamese_model.predict([train_x, train_x, train_x, train_y])\n",
        "\n",
        "      # mean_error = np.mean(compute_distances(predicted_labels, train_y))\n",
        "      mean_error_knn = np.mean(np.abs(predicted_labels_knn - test_y))\n",
        "      mean_error_rf = np.mean(np.abs(predicted_labels_rf - test_y))\n",
        "      mean_error_svm = np.mean(np.abs(predicted_labels_svm - test_y))\n",
        "      mean_error_xgb = np.mean(np.abs(predicted_labels_xgb - test_y))\n",
        "      mean_error_ctb = np.mean(np.abs(predicted_labels_ctb - test_y))\n",
        "      mean_error_ngb = np.mean(np.abs(predicted_labels_ngb - test_y))\n",
        "      # mean_error_lgbm = np.mean(np.abs(predicted_labels_lgbm - test_y))\n",
        "      # print(predicted_labels)\n",
        "      final_knn.append(mean_error_knn)\n",
        "      final_rf.append(mean_error_rf)\n",
        "      final_svm.append(mean_error_svm)\n",
        "      final_xgb.append(mean_error_xgb)\n",
        "      final_ctb.append(mean_error_ctb)\n",
        "      final_ngb.append(mean_error_ngb)\n",
        "      print(f'Mean Error -> KNN {mean_error_knn} -> RF {mean_error_rf} -> SVM {mean_error_svm} -> XgB {mean_error_xgb} -> CtB {mean_error_ctb} -> NGB {mean_error_ngb}')\n",
        "    # print(predicted_labels - test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8KjAzaeFLMJ",
        "outputId": "1d579358-819c-4298-9abb-28315bd3ac1a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test dev -> BLU   flp -> engr1  Ci -> 0\n",
            "9/9 [==============================] - 0s 12ms/step\n",
            "Mean Error -> KNN 11.5625 -> RF 8.61111111111111 -> SVM 11.614583333333334 -> XgB 10.73611111111111 -> CtB 13.044126157407407 -> NGB 10.193521598249482\n",
            "Test dev -> HTC   flp -> engr1  Ci -> 0\n",
            "9/9 [==============================] - 0s 11ms/step\n",
            "Mean Error -> KNN 3.8506944444444446 -> RF 5.878472222222222 -> SVM 3.3541666666666665 -> XgB 4.350694444444445 -> CtB 16.92549189814815 -> NGB 10.868596512445691\n",
            "Test dev -> LG   flp -> engr1  Ci -> 0\n",
            "9/9 [==============================] - 0s 7ms/step\n",
            "Mean Error -> KNN 1.03125 -> RF 3.0208333333333335 -> SVM 1.53125 -> XgB 0.0 -> CtB 16.07494212962963 -> NGB 9.865625466149469\n",
            "Test dev -> MOTO   flp -> engr1  Ci -> 0\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 9.145833333333334 -> RF 18.88888888888889 -> SVM 13.222222222222221 -> XgB 6.576388888888889 -> CtB 17.79759837962963 -> NGB 9.549695333300102\n",
            "Test dev -> OP3   flp -> engr1  Ci -> 0\n",
            "9/9 [==============================] - 0s 8ms/step\n",
            "Mean Error -> KNN 15.75 -> RF 13.819444444444445 -> SVM 14.635416666666666 -> XgB 12.475694444444445 -> CtB 16.403211805555557 -> NGB 11.626642443115612\n",
            "Test dev -> S7   flp -> engr1  Ci -> 0\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 6.829861111111111 -> RF 10.847222222222221 -> SVM 6.690972222222222 -> XgB 9.34375 -> CtB 16.282841435185187 -> NGB 9.762584371985742\n",
            "Test dev -> BLU   flp -> engr1  Ci -> 1\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 8.76736111111111 -> RF 20.40277777777778 -> SVM 8.15625 -> XgB 11.493055555555555 -> CtB 12.680121527777779 -> NGB 20.488204315800544\n",
            "Test dev -> HTC   flp -> engr1  Ci -> 1\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 4.746527777777778 -> RF 7.447916666666667 -> SVM 4.864583333333333 -> XgB 6.267361111111111 -> CtB 16.358506944444443 -> NGB 9.255535875501026\n",
            "Test dev -> LG   flp -> engr1  Ci -> 1\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 2.5034722222222223 -> RF 3.9305555555555554 -> SVM 2.1770833333333335 -> XgB 2.8402777777777777 -> CtB 17.118778935185187 -> NGB 9.426933548374963\n",
            "Test dev -> MOTO   flp -> engr1  Ci -> 1\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 18.75347222222222 -> RF 21.708333333333332 -> SVM 20.84027777777778 -> XgB 11.565972222222221 -> CtB 18.53833912037037 -> NGB 17.723063265464518\n",
            "Test dev -> OP3   flp -> engr1  Ci -> 1\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 11.038194444444445 -> RF 12.371527777777779 -> SVM 11.48611111111111 -> XgB 13.149305555555555 -> CtB 16.12601273148148 -> NGB 11.290041620730433\n",
            "Test dev -> S7   flp -> engr1  Ci -> 1\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 3.96875 -> RF 6.579861111111111 -> SVM 4.090277777777778 -> XgB 4.704861111111111 -> CtB 15.262731481481481 -> NGB 9.435983972776318\n",
            "Test dev -> BLU   flp -> engr1  Ci -> 2\n",
            "9/9 [==============================] - 0s 8ms/step\n",
            "Mean Error -> KNN 10.777777777777779 -> RF 22.055555555555557 -> SVM 10.340277777777779 -> XgB 10.038194444444445 -> CtB 12.393373842592593 -> NGB 21.984138296205145\n",
            "Test dev -> HTC   flp -> engr1  Ci -> 2\n",
            "9/9 [==============================] - 0s 7ms/step\n",
            "Mean Error -> KNN 3.611111111111111 -> RF 4.381944444444445 -> SVM 3.7222222222222223 -> XgB 4.326388888888889 -> CtB 15.713686342592593 -> NGB 9.56018885130052\n",
            "Test dev -> LG   flp -> engr1  Ci -> 2\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 3.6840277777777777 -> RF 6.600694444444445 -> SVM 3.4652777777777777 -> XgB 4.267361111111111 -> CtB 15.435185185185185 -> NGB 9.16045065523506\n",
            "Test dev -> MOTO   flp -> engr1  Ci -> 2\n",
            "9/9 [==============================] - 0s 7ms/step\n",
            "Mean Error -> KNN 5.798611111111111 -> RF 15.104166666666666 -> SVM 9.003472222222221 -> XgB 7.930555555555555 -> CtB 19.10503472222222 -> NGB 10.838287536838363\n",
            "Test dev -> OP3   flp -> engr1  Ci -> 2\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 11.20138888888889 -> RF 13.996527777777779 -> SVM 12.458333333333334 -> XgB 11.822916666666666 -> CtB 16.51287615740741 -> NGB 11.627408869906114\n",
            "Test dev -> S7   flp -> engr1  Ci -> 2\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 5.854166666666667 -> RF 15.784722222222221 -> SVM 7.159722222222222 -> XgB 7.1875 -> CtB 16.2109375 -> NGB 9.331054198403647\n",
            "Test dev -> BLU   flp -> engr1  Ci -> 3\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 10.027777777777779 -> RF 22.086805555555557 -> SVM 9.552083333333334 -> XgB 11.586805555555555 -> CtB 12.547743055555555 -> NGB 20.463747482765054\n",
            "Test dev -> HTC   flp -> engr1  Ci -> 3\n",
            "9/9 [==============================] - 0s 7ms/step\n",
            "Mean Error -> KNN 3.9270833333333335 -> RF 6.375 -> SVM 4.809027777777778 -> XgB 5.534722222222222 -> CtB 15.863425925925926 -> NGB 8.238600582746297\n",
            "Test dev -> LG   flp -> engr1  Ci -> 3\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 2.888888888888889 -> RF 5.902777777777778 -> SVM 2.9895833333333335 -> XgB 4.506944444444445 -> CtB 15.892216435185185 -> NGB 10.188748930748162\n",
            "Test dev -> MOTO   flp -> engr1  Ci -> 3\n",
            "9/9 [==============================] - 0s 10ms/step\n",
            "Mean Error -> KNN 6.357638888888889 -> RF 14.32986111111111 -> SVM 9.565972222222221 -> XgB 10.14236111111111 -> CtB 16.637297453703702 -> NGB 8.985163468619728\n",
            "Test dev -> OP3   flp -> engr1  Ci -> 3\n",
            "9/9 [==============================] - 0s 10ms/step\n",
            "Mean Error -> KNN 10.364583333333334 -> RF 9.572916666666666 -> SVM 10.802083333333334 -> XgB 9.246527777777779 -> CtB 13.19285300925926 -> NGB 10.807750428896659\n",
            "Test dev -> S7   flp -> engr1  Ci -> 3\n",
            "9/9 [==============================] - 0s 10ms/step\n",
            "Mean Error -> KNN 6.048611111111111 -> RF 12.79513888888889 -> SVM 5.9375 -> XgB 8.333333333333334 -> CtB 16.102430555555557 -> NGB 10.565935855276345\n",
            "Test dev -> BLU   flp -> engr1  Ci -> 4\n",
            "9/9 [==============================] - 0s 7ms/step\n",
            "Mean Error -> KNN 10.802083333333334 -> RF 16.78125 -> SVM 9.680555555555555 -> XgB 10.520833333333334 -> CtB 12.982349537037036 -> NGB 16.277615781428224\n",
            "Test dev -> HTC   flp -> engr1  Ci -> 4\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 3.7291666666666665 -> RF 6.815972222222222 -> SVM 3.21875 -> XgB 4.975694444444445 -> CtB 16.530671296296298 -> NGB 9.396922396914707\n",
            "Test dev -> LG   flp -> engr1  Ci -> 4\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 2.5520833333333335 -> RF 3.3506944444444446 -> SVM 2.767361111111111 -> XgB 3.0972222222222223 -> CtB 16.76027199074074 -> NGB 9.373338460598077\n",
            "Test dev -> MOTO   flp -> engr1  Ci -> 4\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 6.038194444444445 -> RF 9.86111111111111 -> SVM 6.3125 -> XgB 8.645833333333334 -> CtB 15.28197337962963 -> NGB 10.63423227216647\n",
            "Test dev -> OP3   flp -> engr1  Ci -> 4\n",
            "9/9 [==============================] - 0s 7ms/step\n",
            "Mean Error -> KNN 11.89236111111111 -> RF 16.97222222222222 -> SVM 14.916666666666666 -> XgB 11.114583333333334 -> CtB 20.092881944444443 -> NGB 12.849158355354197\n",
            "Test dev -> S7   flp -> engr1  Ci -> 4\n",
            "9/9 [==============================] - 0s 7ms/step\n",
            "Mean Error -> KNN 6.576388888888889 -> RF 11.524305555555555 -> SVM 9.659722222222221 -> XgB 6.982638888888889 -> CtB 15.844039351851851 -> NGB 10.244352885747103\n",
            "Test dev -> BLU   flp -> engr1  Ci -> 5\n",
            "9/9 [==============================] - 0s 7ms/step\n",
            "Mean Error -> KNN 11.052083333333334 -> RF 10.42361111111111 -> SVM 10.260416666666666 -> XgB 10.534722222222221 -> CtB 14.143952546296296 -> NGB 9.791571887010592\n",
            "Test dev -> HTC   flp -> engr1  Ci -> 5\n",
            "9/9 [==============================] - 0s 7ms/step\n",
            "Mean Error -> KNN 2.8506944444444446 -> RF 4.413194444444445 -> SVM 2.84375 -> XgB 4.388888888888889 -> CtB 15.50925925925926 -> NGB 11.152744357759413\n",
            "Test dev -> LG   flp -> engr1  Ci -> 5\n",
            "9/9 [==============================] - 0s 7ms/step\n",
            "Mean Error -> KNN 3.4756944444444446 -> RF 4.173611111111111 -> SVM 3.4375 -> XgB 3.9895833333333335 -> CtB 16.64351851851852 -> NGB 9.107364420822725\n",
            "Test dev -> MOTO   flp -> engr1  Ci -> 5\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 6.506944444444445 -> RF 10.32638888888889 -> SVM 7.454861111111111 -> XgB 9.878472222222221 -> CtB 14.305555555555555 -> NGB 11.25565129597652\n",
            "Test dev -> OP3   flp -> engr1  Ci -> 5\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 5.055555555555555 -> RF 7.802083333333333 -> SVM 5.291666666666667 -> XgB 9.791666666666666 -> CtB 16.10011574074074 -> NGB 10.515918395825123\n",
            "Test dev -> S7   flp -> engr1  Ci -> 5\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 5.326388888888889 -> RF 11.09375 -> SVM 6.180555555555555 -> XgB 3.857638888888889 -> CtB 17.65118634259259 -> NGB 9.758111896216434\n",
            "Test dev -> BLU   flp -> engr1  Ci -> 6\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 10.01736111111111 -> RF 20.59027777777778 -> SVM 9.916666666666666 -> XgB 9.36111111111111 -> CtB 12.41087962962963 -> NGB 20.335839279871063\n",
            "Test dev -> HTC   flp -> engr1  Ci -> 6\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 2.8715277777777777 -> RF 4.388888888888889 -> SVM 3.1840277777777777 -> XgB 5.100694444444445 -> CtB 16.594618055555557 -> NGB 9.713371260196759\n",
            "Test dev -> LG   flp -> engr1  Ci -> 6\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 2.9618055555555554 -> RF 4.927083333333333 -> SVM 3.2430555555555554 -> XgB 2.9375 -> CtB 16.780671296296298 -> NGB 9.08845459656711\n",
            "Test dev -> MOTO   flp -> engr1  Ci -> 6\n",
            "9/9 [==============================] - 0s 7ms/step\n",
            "Mean Error -> KNN 11.85763888888889 -> RF 18.8125 -> SVM 14.868055555555555 -> XgB 10.854166666666666 -> CtB 17.94024884259259 -> NGB 12.446807078157484\n",
            "Test dev -> OP3   flp -> engr1  Ci -> 6\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 9.98611111111111 -> RF 11.100694444444445 -> SVM 9.815972222222221 -> XgB 8.597222222222221 -> CtB 12.979890046296296 -> NGB 10.053236487653521\n",
            "Test dev -> S7   flp -> engr1  Ci -> 6\n",
            "9/9 [==============================] - 0s 7ms/step\n",
            "Mean Error -> KNN 5.628472222222222 -> RF 14.381944444444445 -> SVM 5.944444444444445 -> XgB 7.888888888888889 -> CtB 16.150462962962962 -> NGB 9.610076650931907\n",
            "Test dev -> BLU   flp -> engr1  Ci -> 7\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 11.666666666666666 -> RF 19.27777777777778 -> SVM 11.618055555555555 -> XgB 12.479166666666666 -> CtB 12.434027777777779 -> NGB 15.449476034894651\n",
            "Test dev -> HTC   flp -> engr1  Ci -> 7\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 4.520833333333333 -> RF 7.413194444444445 -> SVM 4.385416666666667 -> XgB 4.517361111111111 -> CtB 16.78935185185185 -> NGB 9.102767061453767\n",
            "Test dev -> LG   flp -> engr1  Ci -> 7\n",
            "9/9 [==============================] - 0s 11ms/step\n",
            "Mean Error -> KNN 2.6770833333333335 -> RF 4.597222222222222 -> SVM 2.875 -> XgB 4.892361111111111 -> CtB 15.609230324074074 -> NGB 10.117341015380257\n",
            "Test dev -> MOTO   flp -> engr1  Ci -> 7\n",
            "9/9 [==============================] - 0s 9ms/step\n",
            "Mean Error -> KNN 5.819444444444445 -> RF 12.895833333333334 -> SVM 7.0 -> XgB 10.39236111111111 -> CtB 17.08130787037037 -> NGB 10.00113389337118\n",
            "Test dev -> OP3   flp -> engr1  Ci -> 7\n",
            "9/9 [==============================] - 0s 11ms/step\n",
            "Mean Error -> KNN 12.916666666666666 -> RF 13.118055555555555 -> SVM 14.034722222222221 -> XgB 11.17013888888889 -> CtB 15.375144675925926 -> NGB 11.791243477638375\n",
            "Test dev -> S7   flp -> engr1  Ci -> 7\n",
            "9/9 [==============================] - 0s 11ms/step\n",
            "Mean Error -> KNN 7.125 -> RF 13.024305555555555 -> SVM 7.069444444444445 -> XgB 6.25 -> CtB 17.74262152777778 -> NGB 8.892479287127301\n",
            "Test dev -> BLU   flp -> engr1  Ci -> 8\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 11.725694444444445 -> RF 10.82986111111111 -> SVM 11.621527777777779 -> XgB 13.135416666666666 -> CtB 12.655237268518519 -> NGB 10.083226064348732\n",
            "Test dev -> HTC   flp -> engr1  Ci -> 8\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 2.861111111111111 -> RF 4.729166666666667 -> SVM 2.951388888888889 -> XgB 6.125 -> CtB 16.021846064814813 -> NGB 11.384231663428551\n",
            "Test dev -> LG   flp -> engr1  Ci -> 8\n",
            "9/9 [==============================] - 0s 7ms/step\n",
            "Mean Error -> KNN 2.5208333333333335 -> RF 3.7881944444444446 -> SVM 3.0277777777777777 -> XgB 2.5625 -> CtB 16.51128472222222 -> NGB 8.728792734735457\n",
            "Test dev -> MOTO   flp -> engr1  Ci -> 8\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 7.357638888888889 -> RF 16.041666666666668 -> SVM 10.777777777777779 -> XgB 9.07638888888889 -> CtB 16.982060185185187 -> NGB 10.08137096767978\n",
            "Test dev -> OP3   flp -> engr1  Ci -> 8\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 10.284722222222221 -> RF 10.82638888888889 -> SVM 10.45138888888889 -> XgB 12.40625 -> CtB 15.01388888888889 -> NGB 11.756818709162824\n",
            "Test dev -> S7   flp -> engr1  Ci -> 8\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 3.8229166666666665 -> RF 10.246527777777779 -> SVM 4.045138888888889 -> XgB 5.909722222222222 -> CtB 15.234664351851851 -> NGB 10.893491882494564\n",
            "Test dev -> BLU   flp -> engr1  Ci -> 9\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 10.677083333333334 -> RF 22.4375 -> SVM 10.354166666666666 -> XgB 12.125 -> CtB 12.461082175925926 -> NGB 21.781137473416507\n",
            "Test dev -> HTC   flp -> engr1  Ci -> 9\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 3.2256944444444446 -> RF 5.559027777777778 -> SVM 3.1631944444444446 -> XgB 6.885416666666667 -> CtB 15.60720486111111 -> NGB 10.339179841253944\n",
            "Test dev -> LG   flp -> engr1  Ci -> 9\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 3.111111111111111 -> RF 5.163194444444445 -> SVM 3.420138888888889 -> XgB 4.684027777777778 -> CtB 17.92071759259259 -> NGB 9.67323028434332\n",
            "Test dev -> MOTO   flp -> engr1  Ci -> 9\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 16.11111111111111 -> RF 18.444444444444443 -> SVM 19.381944444444443 -> XgB 9.20486111111111 -> CtB 18.48480902777778 -> NGB 15.90568843339824\n",
            "Test dev -> OP3   flp -> engr1  Ci -> 9\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 14.73611111111111 -> RF 11.739583333333334 -> SVM 14.253472222222221 -> XgB 13.881944444444445 -> CtB 15.164207175925926 -> NGB 10.574196151513478\n",
            "Test dev -> S7   flp -> engr1  Ci -> 9\n",
            "9/9 [==============================] - 0s 6ms/step\n",
            "Mean Error -> KNN 6.055555555555555 -> RF 13.711805555555555 -> SVM 8.684027777777779 -> XgB 5.829861111111111 -> CtB 16.09230324074074 -> NGB 9.75319342486866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Engr0 Head 7\n",
        "# final\n",
        "batch_size = 6\n",
        "averages_knn = []\n",
        "averages_rf = []\n",
        "averages_svm = []\n",
        "averages_xgb = []\n",
        "averages_ctb = []\n",
        "averages_ngb = []\n",
        "\n",
        "for i in range(0, len(final_knn), batch_size):\n",
        "    batch_knn = final_knn[i:i+batch_size]\n",
        "    batch_rf = final_rf[i:i+batch_size]\n",
        "    batch_svm = final_svm[i:i+batch_size]\n",
        "    batch_xgb = final_xgb[i:i+batch_size]\n",
        "    batch_ctb = final_ctb[i:i+batch_size]\n",
        "    batch_ngb = final_ngb[i:i+batch_size]\n",
        "\n",
        "    averages_knn.append(sum(batch_knn) / len(batch_knn))\n",
        "    averages_rf.append(sum(batch_rf) / len(batch_rf))\n",
        "    averages_svm.append(sum(batch_svm) / len(batch_svm))\n",
        "    averages_xgb.append(sum(batch_xgb) / len(batch_xgb))\n",
        "    averages_ctb.append(sum(batch_ctb) / len(batch_ctb))\n",
        "    averages_ngb.append(sum(batch_ngb) / len(batch_ngb))\n",
        "\n",
        "print(f'Engr0 Head -> 7')\n",
        "print(f'KNN -> {averages_knn}')\n",
        "print(f'RF -> {averages_rf}')\n",
        "print(f'SVM -> {averages_svm}')\n",
        "print(f'XGB -> {averages_xgb}')\n",
        "print(f'CTB -> {averages_ctb}')\n",
        "print(f'NGB -> {averages_ngb}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH13gzzHvx_j",
        "outputId": "1388e328-7137-450f-ad70-0e26f6b842c9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN -> [4.9936247723132965, 5.688069216757742, 6.598360655737706, 7.7062841530054635, 6.85336976320583, 6.59927140255009, 5.43351548269581, 7.258652094717669, 6.151639344262295, 7.20582877959927]\n",
            "RF -> [8.670309653916211, 9.419398907103824, 8.429872495446267, 10.704918032786885, 10.517304189435338, 7.786885245901639, 8.360655737704917, 7.907559198542806, 9.044171220400727, 9.191256830601093]\n",
            "SVM -> [5.8902550091074675, 6.995901639344262, 8.398907103825136, 9.60655737704918, 7.637978142076503, 7.072859744990893, 5.6256830601092895, 7.394808743169398, 7.173952641165755, 7.904826958105648]\n",
            "XGB -> [5.2836976320582885, 7.722222222222222, 7.389799635701276, 8.824681238615666, 8.437613843351548, 7.654826958105647, 6.728597449908925, 7.264571948998179, 7.74408014571949, 7.247723132969035]\n",
            "CTB -> [21.352772552181317, 19.70205141986921, 20.652975603929647, 20.225118695691123, 20.557317328077875, 19.488458896951236, 19.77588163277494, 19.697236405984054, 19.328809459822626, 20.08540117650572]\n",
            "NGB -> [10.946570196346153, 11.203111326285162, 11.450162724880364, 13.175490847026767, 13.32069788301924, 11.93610941139054, 11.981761166759929, 12.611536527288292, 10.906178784970734, 11.885130337313422]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Engr1 Head 7\n",
        "batch_size = 6\n",
        "averages_knn = []\n",
        "averages_rf = []\n",
        "averages_svm = []\n",
        "averages_xgb = []\n",
        "averages_ctb = []\n",
        "averages_ngb = []\n",
        "\n",
        "for i in range(0, len(final_knn), batch_size):\n",
        "    batch_knn = final_knn[i:i+batch_size]\n",
        "    batch_rf = final_rf[i:i+batch_size]\n",
        "    batch_svm = final_svm[i:i+batch_size]\n",
        "    batch_xgb = final_xgb[i:i+batch_size]\n",
        "    batch_ctb = final_ctb[i:i+batch_size]\n",
        "    batch_ngb = final_ngb[i:i+batch_size]\n",
        "\n",
        "    averages_knn.append(sum(batch_knn) / len(batch_knn))\n",
        "    averages_rf.append(sum(batch_rf) / len(batch_rf))\n",
        "    averages_svm.append(sum(batch_svm) / len(batch_svm))\n",
        "    averages_xgb.append(sum(batch_xgb) / len(batch_xgb))\n",
        "    averages_ctb.append(sum(batch_ctb) / len(batch_ctb))\n",
        "    averages_ngb.append(sum(batch_ngb) / len(batch_ngb))\n",
        "\n",
        "print(f'Engr1 Head -> 7')\n",
        "print(f'KNN -> {averages_knn}')\n",
        "print(f'RF -> {averages_rf}')\n",
        "print(f'SVM -> {averages_svm}')\n",
        "print(f'XGB -> {averages_xgb}')\n",
        "print(f'CTB -> {averages_ctb}')\n",
        "print(f'NGB -> {averages_ngb}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNPkR-wGt4Dy",
        "outputId": "361f02fa-7163-4302-d661-a3beffb39a66"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Engr1 Head -> 7\n",
            "KNN -> [8.028356481481481, 8.296296296296296, 6.821180555555556, 6.6024305555555545, 6.931712962962962, 5.711226851851852, 7.220486111111111, 7.454282407407407, 6.428819444444444, 8.986111111111112]\n",
            "RF -> [10.177662037037036, 12.073495370370372, 12.987268518518519, 11.84375, 10.884259259259258, 8.038773148148147, 12.366898148148147, 11.721064814814815, 9.410300925925926, 12.842592592592593]\n",
            "SVM -> [8.508101851851851, 8.602430555555555, 7.6915509259259265, 7.276041666666667, 7.759259259259259, 5.911458333333333, 7.828703703703703, 7.830439814814814, 7.145833333333333, 9.876157407407407]\n",
            "XGB -> [7.247106481481481, 8.336805555555555, 7.595486111111111, 8.225115740740742, 7.5561342592592595, 7.073495370370369, 7.456597222222221, 8.283564814814815, 8.202546296296296, 8.768518518518519]\n",
            "CTB -> [16.088035300925927, 16.014081790123456, 15.895182291666666, 15.03932773919753, 16.248697916666668, 15.725597993827163, 15.476128472222221, 15.838614004629628, 15.403163580246913, 15.955054012345679]\n",
            "NGB -> [10.311110954207683, 12.936627099774633, 12.083588067981474, 11.541657791508706, 11.462603358701463, 10.263560375601802, 11.874630892229641, 10.892406794977589, 10.487988670308317, 13.00443760146569]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j3JD5Al9t4sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FDxvRAKht4vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZNOKXOS-t4yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# engr1 head 8\n",
        "final = [6.020833333333333,\n",
        " 8.149305555555555,\n",
        " 0.6736111111111112,\n",
        " 14.916666666666666,\n",
        " 10.930555555555555,\n",
        " 9.756944444444445,\n",
        " 9.725694444444445,\n",
        " 3.6493055555555554,\n",
        " 2.46875,\n",
        " 23.5,\n",
        " 21.03472222222222,\n",
        " 6.420138888888889,\n",
        " 6.701388888888889,\n",
        " 5.913194444444445,\n",
        " 3.454861111111111,\n",
        " 12.538194444444445,\n",
        " 23.447916666666668,\n",
        " 4.149305555555555,\n",
        " 8.444444444444445,\n",
        " 3.6944444444444446,\n",
        " 3.90625,\n",
        " 6.826388888888889,\n",
        " 20.774305555555557,\n",
        " 5.440972222222222,\n",
        " 6.774305555555555,\n",
        " 2.986111111111111,\n",
        " 2.125,\n",
        " 16.770833333333332,\n",
        " 20.21527777777778,\n",
        " 7.03125,\n",
        " 11.604166666666666,\n",
        " 6.020833333333333,\n",
        " 2.6805555555555554,\n",
        " 11.086805555555555,\n",
        " 21.15972222222222,\n",
        " 5.760416666666667,\n",
        " 7.586805555555555,\n",
        " 6.420138888888889,\n",
        " 2.6840277777777777,\n",
        " 14.659722222222221,\n",
        " 8.385416666666666,\n",
        " 6.329861111111111,\n",
        " 10.34375,\n",
        " 4.604166666666667,\n",
        " 2.5625,\n",
        " 8.631944444444445,\n",
        " 7.597222222222222,\n",
        " 4.229166666666667,\n",
        " 6.673611111111111,\n",
        " 3.0243055555555554,\n",
        " 2.0555555555555554,\n",
        " 16.052083333333332,\n",
        " 12.60763888888889,\n",
        " 4.201388888888889,\n",
        " 4.652777777777778,\n",
        " 3.6875,\n",
        " 2.9722222222222223,\n",
        " 23.5,\n",
        " 15.59375,\n",
        " 15.260416666666666]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-kvRgf49kdJ",
        "outputId": "f435e507-2645-4822-84f5-51e44cedf54b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6.020833333333333,\n",
              " 8.149305555555555,\n",
              " 0.6736111111111112,\n",
              " 14.916666666666666,\n",
              " 10.930555555555555,\n",
              " 9.756944444444445,\n",
              " 9.725694444444445,\n",
              " 3.6493055555555554,\n",
              " 2.46875,\n",
              " 23.5,\n",
              " 21.03472222222222,\n",
              " 6.420138888888889,\n",
              " 6.701388888888889,\n",
              " 5.913194444444445,\n",
              " 3.454861111111111,\n",
              " 12.538194444444445,\n",
              " 23.447916666666668,\n",
              " 4.149305555555555,\n",
              " 8.444444444444445,\n",
              " 3.6944444444444446,\n",
              " 3.90625,\n",
              " 6.826388888888889,\n",
              " 20.774305555555557,\n",
              " 5.440972222222222,\n",
              " 6.774305555555555,\n",
              " 2.986111111111111,\n",
              " 2.125,\n",
              " 16.770833333333332,\n",
              " 20.21527777777778,\n",
              " 7.03125,\n",
              " 11.604166666666666,\n",
              " 6.020833333333333,\n",
              " 2.6805555555555554,\n",
              " 11.086805555555555,\n",
              " 21.15972222222222,\n",
              " 5.760416666666667,\n",
              " 7.586805555555555,\n",
              " 6.420138888888889,\n",
              " 2.6840277777777777,\n",
              " 14.659722222222221,\n",
              " 8.385416666666666,\n",
              " 6.329861111111111,\n",
              " 10.34375,\n",
              " 4.604166666666667,\n",
              " 2.5625,\n",
              " 8.631944444444445,\n",
              " 7.597222222222222,\n",
              " 4.229166666666667,\n",
              " 6.673611111111111,\n",
              " 3.0243055555555554,\n",
              " 2.0555555555555554,\n",
              " 16.052083333333332,\n",
              " 12.60763888888889,\n",
              " 4.201388888888889,\n",
              " 4.652777777777778,\n",
              " 3.6875,\n",
              " 2.9722222222222223,\n",
              " 23.5,\n",
              " 15.59375,\n",
              " 15.260416666666666]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# engr1 head 8\n",
        "final\n",
        "batch_size = 6\n",
        "averages = []\n",
        "\n",
        "for i in range(0, len(final), batch_size):\n",
        "    batch = final[i:i+batch_size]\n",
        "    batch_average = sum(batch) / len(batch)\n",
        "    averages.append(batch_average)\n",
        "\n",
        "print(averages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJmHkRXi2h4V",
        "outputId": "41e3360c-5daf-4787-d580-03f79c09295d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8.40798611111111, 11.133101851851853, 9.367476851851853, 8.18113425925926, 9.31712962962963, 9.71875, 7.677662037037035, 6.328125, 7.435763888888888, 10.944444444444445]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(final).mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HzbJBRZ3DGF",
        "outputId": "95848836-35c1-4522-a273-d7e677a6b18e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.851157407407408"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# engr0 head 8\n",
        "final = [3.5737704918032787,\n",
        " 5.721311475409836,\n",
        " 0.6639344262295082,\n",
        " 9.357923497267759,\n",
        " 4.991803278688525,\n",
        " 3.9344262295081966,\n",
        " 5.398907103825136,\n",
        " 5.516393442622951,\n",
        " 3.4262295081967213,\n",
        " 14.245901639344263,\n",
        " 5.969945355191257,\n",
        " 5.256830601092896,\n",
        " 5.8497267759562845,\n",
        " 7.595628415300546,\n",
        " 4.03551912568306,\n",
        " 21.724043715846996,\n",
        " 5.35792349726776,\n",
        " 4.64207650273224,\n",
        " 8.997267759562842,\n",
        " 6.579234972677596,\n",
        " 4.622950819672131,\n",
        " 10.210382513661202,\n",
        " 6.5710382513661205,\n",
        " 4.248633879781421,\n",
        " 8.37704918032787,\n",
        " 6.601092896174864,\n",
        " 4.23224043715847,\n",
        " 18.579234972677597,\n",
        " 6.243169398907104,\n",
        " 4.396174863387978,\n",
        " 9.866120218579235,\n",
        " 4.368852459016393,\n",
        " 4.89344262295082,\n",
        " 7.795081967213115,\n",
        " 6.344262295081967,\n",
        " 4.409836065573771,\n",
        " 4.112021857923497,\n",
        " 3.5327868852459017,\n",
        " 2.1639344262295084,\n",
        " 7.73224043715847,\n",
        " 4.497267759562842,\n",
        " 6.576502732240437,\n",
        " 8.636612021857923,\n",
        " 6.226775956284153,\n",
        " 3.2814207650273226,\n",
        " 6.631147540983607,\n",
        " 6.612021857923497,\n",
        " 3.9508196721311477,\n",
        " 8.6775956284153,\n",
        " 6.319672131147541,\n",
        " 2.6311475409836067,\n",
        " 7.076502732240437,\n",
        " 6.939890710382514,\n",
        " 4.721311475409836,\n",
        " 8.521857923497267,\n",
        " 4.770491803278689,\n",
        " 5.866120218579235,\n",
        " 7.745901639344262,\n",
        " 6.286885245901639,\n",
        " 4.6502732240437155]"
      ],
      "metadata": {
        "id": "fsV-RFZl1KgU"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 6\n",
        "averages = []\n",
        "\n",
        "for i in range(0, len(final), batch_size):\n",
        "    batch = final[i:i+batch_size]\n",
        "    batch_average = sum(batch) / len(batch)\n",
        "    averages.append(batch_average)\n",
        "\n",
        "print(averages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBql0kI_9pk_",
        "outputId": "52941072-b4ee-4f57-b3e2-3a128703e3c8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.707194899817851, 6.6357012750455375, 8.200819672131148, 6.871584699453552, 8.071493624772314, 6.27959927140255, 4.76912568306011, 5.889799635701276, 6.0610200364298725, 6.306921675774134]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(final).mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xyJ9w5Y1d5Q",
        "outputId": "06d1f414-1070-45ee-e138-864dd4d70905"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.379326047358834"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "_6W-UixivIQC",
        "outputId": "70be607a-5d6c-478e-b0b0-45a61edad490"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7949889fd28e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'predicted_labels' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y[2]"
      ],
      "metadata": {
        "id": "XzrJGmXtuy_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels.shape"
      ],
      "metadata": {
        "id": "6ALwY0UQzRcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_y)"
      ],
      "metadata": {
        "id": "1iGk7YiWuYfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred.shape"
      ],
      "metadata": {
        "id": "F8C8jz9gcM8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_train_output[0].shape"
      ],
      "metadata": {
        "id": "681p7aj-tOJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x[0]"
      ],
      "metadata": {
        "id": "CkIhMqUUtYYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_x[0]"
      ],
      "metadata": {
        "id": "dZubmva3EU3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.shape"
      ],
      "metadata": {
        "id": "7kwjve8DEZHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ngboost import NGBClassifier\n",
        "# from ngboost.distns import Bernoulli\n",
        "# from ngboost import NGBRegressor\n",
        "\n",
        "# ngb = NGBRegressor().fit(encoded_train_output, train_y)\n",
        "# Y_preds = ngb.predict(encoded_train_output)\n",
        "\n",
        "\n",
        "\n",
        "# ngb = NGBClassifier(Dist=Bernoulli)\n",
        "# ngb.fit(encoded_train_output, train_y)\n",
        "\n",
        "# preds = ngb.pred_dist(encoded_train_output)"
      ],
      "metadata": {
        "id": "fYmDxbJJKGmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ngb = NGBClassifier().fit(encoded_train_output, train_y)"
      ],
      "metadata": {
        "id": "brN-TxToh-DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade ngboost"
      ],
      "metadata": {
        "id": "wfJ0zoWKlWfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "npr4mmsGnqFy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}